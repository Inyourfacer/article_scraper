{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Use Selenium to open webpages that use java script rendering ###############\n",
    "from selenium import webdriver\n",
    "####### Use Pandas to manage and arrange data #################\n",
    "import pandas as pd\n",
    "####### Use BeautifulSoup to read and navigate the DOM ################\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "######### Use Regex to clean up some of the elements we scrape #################\n",
    "import re\n",
    "########## Use Numpy for data manipulation ###############\n",
    "import numpy as np\n",
    "########## Use the nltk library to tokenize and analyse text results ##########\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "############# Use Networks for graphing of text analysis #############\n",
    "import networkx as nx\n",
    "############# use Matplotlib and WordCloud to create and display a wordcloud ################\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "############## Download these files if they are not upto date #############\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# First we ask user for input of what topics should be searched/how many pages of Google reuslts they would like/ and how recent the results should be limited to ######################\n",
    "results = []\n",
    "pages = []\n",
    "allsellers = []\n",
    "links = []\n",
    "terms = input('Enter what you would like to search : ')\n",
    "terms = terms.split(', ')\n",
    "length = input('How many pages would you like to search : ')\n",
    "length = int(length)\n",
    "recency = input('How recently do you want results from? Select h/d/w/m/y :  ')\n",
    "recency = str(recency)\n",
    "i = 0\n",
    "############# for each search term entered run the for loop to gather the search results #########\n",
    "############# 'i' represents the number of pages requested then a decreasing while loop will gather the needed reults ###############\n",
    "for t in terms:\n",
    "    while i < length:\n",
    "        i += 1\n",
    "        pnum = str(1-i)\n",
    "        driver = webdriver.Chrome()\n",
    "        base = 'https://www.google.com/search?q='\n",
    "        time = ('&tbs=qdr:'+recency)\n",
    "        depth = '&start='+pnum+'0'\n",
    "        url = base+t+time+depth\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(url)\n",
    "        soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        results=soup.find_all('div',{'class':'r'})\n",
    "        \n",
    "        for link in results:\n",
    "            links.extend(link.find_all('a'))\n",
    "        links=[re.sub(r'.*href\\=\\\"',r'',str(a))for a in links]\n",
    "        links=[re.sub(r'\".*',r'',str(a))for a in links]\n",
    "        while '#' in links:\n",
    "            links.remove('#')\n",
    "        driver.quit()\n",
    "############ once we have gathered all of the links from the pages into a list we can clean them and remove unwanted domians ###########        \n",
    "links = [re.sub(r'.*https',r'https',str(a))for a in links]\n",
    "links = [re.sub(r'\\/search\\?q\\=related\\:',r'https\\:\\/\\/',str(a))for a in links]\n",
    "links = [x for x in links if 'wikipedia' not in x]\n",
    "links = [x for x in links if 'encyclopedia' not in x]\n",
    "print(len(links))\n",
    "links = [re.sub(r'\\+\\&amp.*',r'',str(a))for a in links]\n",
    "links=set(links)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ this function will look at the parent element tags and filter out ones that are irrelivant #############\n",
    "def visable(element):\n",
    "    if element.parent.name in ['style', 'script', ['document'], 'head', 'title', 'meta']:\n",
    "        return False\n",
    "    elif re.match(r'<!--.*-->',str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Now that the links have been cleaned we can open each page and scrape the 'visible' elements for the text #################\n",
    "corpus = []\n",
    "art = links\n",
    "for url in art:\n",
    "    ################## we first try to access the page with BeautifulSoup because it is faster #############\n",
    "    ################## if it errors out we will try to open it with Selenium to render javascript ############\n",
    "    ################## if both methods fail we will skip the page ###############\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        data = soup.findAll(test=True)\n",
    "        result = filter(visable, data)\n",
    "        result = [re.sub(r'\\n',r'',str(a))for a in result]\n",
    "        result = [re.sub(r'.*src\\=.*',r'',str(a))for a in result]\n",
    "        result = [re.sub(r'\\t',r'',str(a))for a in result]\n",
    "        result = [re.sub(r'\\<.*\\>',r'',str(a))for a in result]\n",
    "        result = [re.sub(r'\\{.*\\}',r'',str(a))for a in result]\n",
    "        while '' in result:\n",
    "            result.remove('')\n",
    "        corpus.appen(result)\n",
    "    except:\n",
    "        driver=webdriver.Chrome()\n",
    "        driver.implicitly_wait(30)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            driver.quit()\n",
    "            data = soup.findAll(text = True)\n",
    "\n",
    "            result = filter(visable, data)\n",
    "            result = [re.sub(r'\\n',r'',str(a))for a in result]\n",
    "            result = [re.sub(r'.*src\\=.*',r'',str(a))for a in result]\n",
    "            result = [re.sub(r'\\t',r'',str(a))for a in result]\n",
    "            result = [re.sub(r'\\<.*\\>',r'',str(a))for a in result]\n",
    "            result = [re.sub(r'\\{.*\\}',r'',str(a))for a in result]\n",
    "            while '' in result:\n",
    "                result.remove('')\n",
    "            corpus.append(result)\n",
    "        except:\n",
    "            result = 'Bad URL at : ' + url\n",
    "            corpus.append(result)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# in an effort to remove visible text that is unrealted to the page content, like social media links and ads, we filter out strings less than 100 charcters. ##############\n",
    "############# NOTE: this may also remove <li> elements or other relevant but very short elements on the page ###############\n",
    "final = []\n",
    "for i in text:\n",
    "    temp = []\n",
    "    for x in i:\n",
    "#         print(len(x))\n",
    "        s = len(x)\n",
    "        if s > 100:\n",
    "#             y = [re.sub(r'.*',r'',str(a))for a in x]\n",
    "#             while '' in y:\n",
    "#                 y.remove('')\n",
    "            temp.append(x)\n",
    "        else:\n",
    "            x=1\n",
    "    final.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Create a single string per page ##############\n",
    "df = [''.join(x)for x in final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Tokenize the results ########################\n",
    "sentences = []\n",
    "for s in df:\n",
    "  sentences.append(sent_tokenize(s))\n",
    "\n",
    "sentences = [y for x in sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Extract word vectors from GloVe file ##################3\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### remove punctuations, numbers and special characters ####################\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "############ make alphabets lowercase ##################\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## import stopwords  list (or create your own)#####################\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### this function will remove stopwords ####################\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## remove stopwords from the sentences ######################\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### create a vector for the sentences using the GloVe vector library and pulled text ##############\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## create a similarity matrix ################\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## perform cosine-similarity for vectors #############\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### rank pages based on cosine similarity ##########\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Extract top 10 sentences as the summary #################\n",
    "for i in range(10):\n",
    "  print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### create word cloud for ranked sentences #############\n",
    "wctext = ranked_sentences\n",
    "wctext = str(wctext)\n",
    "wordcloud = WordCloud().generate(wctext)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
